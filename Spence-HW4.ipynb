{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "256b26d4-70a8-43f9-a427-04de33772704",
   "metadata": {},
   "source": [
    "# Sarah Spence -HW4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "834be5b8-40d1-48e2-81b9-5977d4efcc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import common_helpers as common\n",
    "from machine_learning import RBF_neural_network as RBF\n",
    "\n",
    "iris_df = pd.read_csv(\"data_files/iris.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bca7ef-812c-46ac-9dfd-61d1ac12b844",
   "metadata": {},
   "source": [
    "## 1. Problem 1 - Module 9 Note this is a Collaborative Problem\n",
    "50 Points Total\n",
    "\n",
    "The RBF NN algorithm is to be used for classification of the Iris data set. In this problem thefollowing subproblems are to be completed:\n",
    "    \n",
    "### (a) [10 points] Using the RBF in Section 2 of the Neural Network documentation develop in pseudocode an RBF NN (no bias) system to accomplish the following steps:\n",
    "\n",
    "#### i. Develop (pseudocode) an RBF NN train/training function.\n",
    "\n",
    "def RBF_NN_Train(data, targets, spread):\n",
    "    \n",
    "    1. model_output < a dictionary to store model parameters\n",
    "    cost: c1  time: 1\n",
    "    2. observations, features <- get the rows and columns of the data\n",
    "    cost: c2  time: 1\n",
    "    3. H <- create a matrix of zeros of size observations X observations\n",
    "    cost: c3  time: n^2 where n is the number of observations\n",
    "    for i in range(observations):\n",
    "        for j in range(observations):\n",
    "            4. weight <- data[i, :]\n",
    "            cost: c4  time: 1 * n^2\n",
    "            5. calc_1 <- perform matrix multiplication on row j of the dataset - weigth and row j - weight transposed\n",
    "            matrix multiplication = m^2.3728596 where m is the number of features\n",
    "            transpose = 1\n",
    "            cost: c5  time: (m^2.3728596 + 1) * n^2\n",
    "            6. H[j, i] <- calculate the exponential of (-calc_1)/(2 * spread^2)\n",
    "            exponential = k where k is the number of elements in calc_1\n",
    "            2 * spread^2 = 1\n",
    "            cost: c6  time: (k + 1) * n^2\n",
    "    \n",
    "    \n",
    "    7. w_hat <- np.matmul(np.matmul(np.linalg.pinv(np.matmul(H.T, H)), H.T), targets)\n",
    "    matrix multiplication = n^2.3728596\n",
    "    pseudo inverse = n*m^2\n",
    "    cost: c7  time: n^2.3728596 + n^2.3728596 + + n^2.3728596 + n*m^2 + 1 + 1  where n is the number of observations and m is the number of features\n",
    "    8. yt <- perform matrix multiplication on H and w_hat, then transpose the result\n",
    "    cost: c8  time: n^2.3728596 + 1\n",
    "    9. y_pred <- create an array of 1s the length of the target data\n",
    "    cost: c9  time: n\n",
    "    \n",
    "    10. y_pred_indices <- find all indices where the value is less than 0\n",
    "    cost: c10  time: n\n",
    "    \n",
    "    11. update all indices to be -1 where the value is less than zero\n",
    "    cost: c11  time: n\n",
    "    \n",
    "    12. correct_pred <- find all correct predictions by comparing y_pred to the labels\n",
    "    cost: c12  time: n\n",
    "    \n",
    "    13. pred_error <- calculate the error rate by 1 - accuracy\n",
    "    cost: c13  time: 1\n",
    "    \n",
    "    14. model_output[\"w_hat\"] <- w_hat\n",
    "    cost: c14  time: 1\n",
    "    15. model_output[\"w\"] <- data\n",
    "    cost: c15  time: 1\n",
    "    16. model_output[\"spread\"] <- spread\n",
    "    cost: c16  time: 1\n",
    "    17. model_output[\"error\"] <- pred_error\n",
    "    cost: c17  time: 1\n",
    "    \n",
    "    return model_output\n",
    "\n",
    "\n",
    "T(n) = c1(1) + c2(1) + c3(n^2) + c4(1 * n^2) + c5((m^2.3728596 + 1) * n^2) + c6((k + 1) * n^2) + c7(n^2.3728596 + n^2.3728596 + + n^2.3728596 + n*m^2 + 1 + 1) + c8(n^2.3728596 + 1) + c9(n) + c10(n) + c11(n) + 12(n) + c13(1) + c14(1) + c15(1) + c16(n) + c17(n)\n",
    "\n",
    "O-notation = O(m^2.3728596+ n^2.3728596 + n*m^2) where n is the number of examples and m is the number of features \n",
    "\n",
    "#### ii. Develop (pseudocode) an RBF NN test/testing function.\n",
    "\n",
    "def RBF_NN_Classify(data, model):\n",
    "  \n",
    "    1. obs_1, features_1 <- get the shape of the data\n",
    "    cost: c1  time: 1\n",
    "    2. obs_2, features_2 <- get the shape of the model weights\n",
    "    cost: c2  time: 1\n",
    "    \n",
    "    \n",
    "    3. H <- create a matrix of zeros of size obs_1 X obs_2\n",
    "    cost: c3  time: n*m where n is obs_1 and m is obs_2\n",
    "    \n",
    "    \n",
    "    for i in range(obs_2):\n",
    "        for j in range(obs_1):\n",
    "            4. w <- model[\"w\"][i, :]\n",
    "            cost: c4  time: n*m*1\n",
    "            5. calc_1 <- perform matrix multiplication on row j of the dataset - w and row j - w transposed\n",
    "            matrix multiplication = m^2.3728596 where m is the number of features\n",
    "            transpose = 1\n",
    "            cost: c5  time: (m^2.3728596 + 1) * n*m\n",
    "            6. H[j, i] <- calculate the exponential of (-calc_1)/(2 * model.spread^2)\n",
    "            exponential = k where k is the number of elements in calc_1\n",
    "            2 * spread^2 = 1\n",
    "            cost: c6  time: (k + 1) * n*m\n",
    "            \n",
    "    7. y <- perform matrix multiplication on H and model.w_hat and take the tranpose of this result\n",
    "    cost: c7  time: n^2.3728596 + 1\n",
    "    \n",
    "    8. y_hat <- create a matrix of zeros of 1 X obs_1\n",
    "    cost: c8  time: n where n is obs_1\n",
    "\n",
    "    for i in range(obs_1):\n",
    "        for j in range(obs_2):\n",
    "            9. w <- model[\"w\"][j, :]\n",
    "            cost: c9  time: 1 * mn\n",
    "            10. w_hat <- model[\"w_hat\"][j]\n",
    "            cost: c10  time: 1 * mn\n",
    "            11. y_hat[0][i] <- y_hat[0][i] + w_hat * np.exp(-(np.matmul((data[i, :]-w), (data[i, :]-w).T))/ (2 * np.power(model[\"spread\"], 2)))\n",
    "            cost: c11  time: c5 + c6\n",
    "  \n",
    "    12. y_pred <- Create an array of 1s the length of the data\n",
    "    cost: c12  time: n\n",
    "   \n",
    "    13. y_pred_indices <- Get all indices with negative predictions\n",
    "    cost: c13  time: n\n",
    "    \n",
    "    14. Update all indices where a prediction was negative\n",
    "    c:14  time: n\n",
    "    \n",
    "    return y, y_pred\n",
    "\n",
    "T(n) = c1(1) + c2(1) + c3(nm) + c4(nm * 1) + c5((m^2.3728596 + 1) * n*m) + c6((k + 1) * n*m) + c7(n^2.3728596 + 1) + c8(n) + c9(mn * 1) + c10(mn * 1) + c11(c5 + c6) + c12(n) + c13(n) + c14(n)\n",
    "\n",
    "O-notation = O((m^2.3728596 * nm) where n is obs_1 and m is obs_2\n",
    "\n",
    "def RBF_NN_Test_Function(data, models, labels):\n",
    "\n",
    "    1. y_list <- define a list for ys\n",
    "    cost: c1  time: 1\n",
    "    2. y_pred_list <- define a list of y predictions\n",
    "    cost: c2  time: 1\n",
    "    \n",
    "    for model in models:\n",
    "        3. y, y_pred <- RBF_NN_Classify(data, model)\n",
    "        cost: c3  time: (m^2.3728596 * nm) * p where p is the number of models\n",
    "        4. append y to y_list\n",
    "        cost: c4  time: 1 * p\n",
    "        5. append y_pred to y_pred_list\n",
    "        cost: c5  time: 1 * p\n",
    "    \n",
    "    6. temp <- create an empty numpy array\n",
    "    cost: c6  time: 1\n",
    "    \n",
    "    for i in range(len(y_list)):\n",
    "        7. temp <- stack columns temp and y_list[i]\n",
    "        cost: c7  time: l where l is the length of y_list\n",
    "    \n",
    "    8. temp <- temp.T\n",
    "    cost: c8  time: 1\n",
    "    \n",
    "    9. y_pred <- find the max value across all rows of temp for every column to make predictions\n",
    "    cost: c9  time: n*m where n is the number of rows and m is the number of cols in temp\n",
    "    \n",
    "    10. correct_pred <- 0\n",
    "    cost: c10  time: 1\n",
    "    for i, val in enumerate(y_pred):\n",
    "        if y_pred[i] == labels[i]:\n",
    "            11. correct_pred += 1\n",
    "            cost: c11  time: n where n is the length of y_pred\n",
    "            \n",
    "    12. accuracy <- correct_pred/(len(data)*100)\n",
    "    cost: c12  time: 1\n",
    "    \n",
    "    return y_pred, accuracy\n",
    "    \n",
    "T(N): c1(1) + c2(1) + c3((m^2.3728596 * nm) * p) + c4(p) + c5(p) + c6(1) + c7(l) + c8(1) + c9(mn) + c10(1) + c11(n) + c12(1)\n",
    "\n",
    "O-notation: O((m^2.3728596 * nm) * p) where m is the number of weights in the models, n is the number of observations in the data, and p is the number of models used\n",
    "\n",
    "### (b) Optional no need to discuss collaboratively - Analyze your design\n",
    "#### i. Calculate the running time of the system above in O-notation.\n",
    "\n",
    "Training algorithm: O(m^2.3728596+ n^2.3728596 + n*m^2) where n is the number of examples and m is the number of features \n",
    "\n",
    "Testing algorithm: O((m^2.3728596 * nm) * p) where m is the number of weights in the models, n is the number of observations in the data, and p is the number of models used\n",
    "\n",
    "#### ii. Calculate the total running time of the above system as T (n) with each line of pseudocode or code accounted for.\n",
    "\n",
    "Training algorithm: c1(1) + c2(1) + c3(n^2) + c4(1 * n^2) + c5((m^2.3728596 + 1) * n^2) + c6((k + 1) * n^2) + c7(n^2.3728596 + n^2.3728596 + + n^2.3728596 + n*m^2 + 1 + 1) + c8(n^2.3728596 + 1) + c9(n) + c10(n) + c11(n) + 12(n) + c13(1) + c14(1) + c15(1) + c16(n) + c17(n)\n",
    "\n",
    "Testing algorithm: c1(1) + c2(1) + c3((m^2.3728596 * nm) * p) + c4(p) + c5(p) + c6(1) + c7(l) + c8(1) + c9(mn) + c10(1) + c11(n) + c12(1)\n",
    "\n",
    "#### iii. How does the total running time T (n) compare to the running time in O-notation?\n",
    "\n",
    "The T(n) calculation shows the total running time of the algorithm and does not drop lower order terms. The O notation calculations retain only the highest order terms to calculate the worst case running time of the algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a372fb1-7bf2-445a-92a7-eb9ca993e8ad",
   "metadata": {},
   "source": [
    "### (c) [40 points] Implement your design using Python or R\n",
    "#### i. Implement your developed RBF NN.\n",
    "#### See machine_learning/RBF_neural_network.py for the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588a0596-c903-43eb-8cd9-aa1ee0ff982b",
   "metadata": {},
   "source": [
    "##### A. Train three two class models using the Iris data set as input training data, the Iris data will need to be reconfigured as a one-vs-all or one-vs-one data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7771a8c2-7c72-455e-9953-2a73313eb7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= np.array(iris_df.iloc[:, :-1])\n",
    "\n",
    "# setosa vs. rest\n",
    "model_1_labels = [1 for i in range(50)] + [-1 for i in range(100)]\n",
    "model_1 = RBF.RBF_NN_Train(data, model_1_labels, 0.1)\n",
    "\n",
    "# versicolor vs. rest\n",
    "model_2_labels = [-1 for i in range(50)] + [1 for i in range(50)] + [-1 for i in range(50)]\n",
    "model_2 = RBF.RBF_NN_Train(data, model_2_labels, 0.1)\n",
    "\n",
    "# virginica vs. rest\n",
    "model_3_labels = [-1 for i in range(100)] + [1 for i in range(50)]\n",
    "model_3 = RBF.RBF_NN_Train(data, model_3_labels, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d6f479-9d4f-42cd-8740-47e769e9045c",
   "metadata": {},
   "source": [
    "##### B. Process the test data set to determine which class each test observation belongs to, in this problem you will simply use all 150 observations as your test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7d0d317-4856-485a-a16a-ff2b9535670b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\n",
      "Predicted   0   1   2\n",
      "Actual               \n",
      "0          50   0   0\n",
      "1           0  50   0\n",
      "2           0   0  50\n",
      "\n",
      "The model accuracy with a spread of 0.1 was:\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "iris_labels = [0 for i in range(50)] + [1 for i in range(50)] + [2 for i in range(50)]\n",
    "predictions, accuracy = RBF.RBF_NN_Test_Function(data, [model_1, model_2, model_3], iris_labels)\n",
    "\n",
    "common.confusion_matrix(iris_labels, predictions)\n",
    "print(f\"The model accuracy with a spread of 0.1 was:\\n{accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "622c7544-bb81-45f7-ae3a-43c1cdb68693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\n",
      "Predicted   0   1   2\n",
      "Actual               \n",
      "0          50   0   0\n",
      "1           0  50   0\n",
      "2           0   0  50\n",
      "\n",
      "The model accuracy with a spread of 0.5 was:\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "#Test again using a spread of 0.5\n",
    "\n",
    "# setosa vs. rest\n",
    "model_1 = RBF.RBF_NN_Train(data, model_1_labels, 0.5)\n",
    "\n",
    "# versicolor vs. rest\n",
    "model_2 = RBF.RBF_NN_Train(data, model_2_labels, 0.5)\n",
    "\n",
    "# virginica vs. rest\n",
    "model_3 = RBF.RBF_NN_Train(data, model_3_labels, 0.5)\n",
    "\n",
    "iris_labels = [0 for i in range(50)] + [1 for i in range(50)] + [2 for i in range(50)]\n",
    "predictions, accuracy = RBF.RBF_NN_Test_Function(data, [model_1, model_2, model_3], iris_labels)\n",
    "\n",
    "common.confusion_matrix(iris_labels, predictions)\n",
    "print(f\"The model accuracy with a spread of 0.5 was:\\n{accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eb3fa1d-f3c6-4279-a103-934b63d24d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\n",
      "Predicted   0   1   2\n",
      "Actual               \n",
      "0          50   0   0\n",
      "1           0  49   1\n",
      "2           0   0  50\n",
      "\n",
      "The model accuracy with a spread of 4 was:\n",
      "99.33%\n"
     ]
    }
   ],
   "source": [
    "#Test again using a spread of 4\n",
    "\n",
    "# setosa vs. rest\n",
    "model_1 = RBF.RBF_NN_Train(data, model_1_labels, 4)\n",
    "\n",
    "# versicolor vs. rest\n",
    "model_2 = RBF.RBF_NN_Train(data, model_2_labels, 4)\n",
    "\n",
    "# virginica vs. rest\n",
    "model_3 = RBF.RBF_NN_Train(data, model_3_labels, 4)\n",
    "\n",
    "iris_labels = [0 for i in range(50)] + [1 for i in range(50)] + [2 for i in range(50)]\n",
    "predictions, accuracy = RBF.RBF_NN_Test_Function(data, [model_1, model_2, model_3], iris_labels)\n",
    "\n",
    "common.confusion_matrix(iris_labels, predictions)\n",
    "print(f\"The model accuracy with a spread of 4 was:\\n{round(accuracy, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4c0892-3722-461c-87ae-4d29a5a8962f",
   "metadata": {},
   "source": [
    "#### ii. What is the classification accuracy of your design?\n",
    "\n",
    "The classification accuracy of my design was 100% when testing against the iris dataset with a spread of 0.1 and a spread of 0.5. \n",
    "\n",
    "#### iii. If you had any misclassifications what was the cause of this, e.g, did the spread have an impact.\n",
    "\n",
    "There were no misclassifications when testing agaist the iris dataset with smaller spreads of 0.1 and 0.5. Testing with a substantially larger spread of 4 caused the model to make 1 misclassification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5082105d-80f4-4664-84be-2b6648d96653",
   "metadata": {},
   "source": [
    "## 2. Problem 2 - Module 10/11 Note this is not a Collaborative Problem\n",
    "50 Points Total\n",
    "\n",
    "![image](./data_files/images/Figure_7.jpg)\n",
    "\n",
    "Choose one algorithm from Figures 2 or 3, manually show the subtree with X taking the first move at the bottom row middle column shown in Figure 1. Label each state with it corresponding values of each variable in the algorithm, e.g., state, player, v, move, and game.Actions(). The goal is to show visually how the algorithm is exploring the node in level 0 and exploring in a downward manner reaching the leaf nodes. In the HW 4 Module, please reference the Introduction to Adversarial Search and Game Play.pdf\n",
    "\n",
    "![image](./data_files/images/Figure_8.jpg)\n",
    "\n",
    "Figure 2: An algorithm for calculating the optimal move using MiniMax - the move that leads to a terminal state with maximum utility, under the assumption that the opponent plays to minimize utility. The functions Max-Value and Min-Value go through the whole game tree, all the way to the leaves, to determine the backed-up value of a state and the move to get there.\n",
    "\n",
    "![image](./data_files/images/Figure_9.jpg)\n",
    "\n",
    "Figure 3: The alpha-beta search algorithm. Notice that these functions are the same as the Min-max-Search functions in Figure 2, except that we maintain bounds in the variables α and β, and use then to cut off search when a value is outside the bounds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde696eb-99ae-4a00-a828-c5709bd61b8c",
   "metadata": {},
   "source": [
    "### The image below shows the Min-Max Search example starting with player X taking the first move at position 5.\n",
    "\n",
    "![image](./data_files/images/Min-Max-Search-Example.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
